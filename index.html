<html>
	<head>
		<style type="text/css">
			body{font-family: 'Work sans' sans-serif;}
			p{display: inline-block;}
			img{display: block;}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-weight: bold;font-size: 24px;padding: 1%;}
			.section{position: relative;width: 90%;margin: auto;padding: 2%;}
			.subsection{position: relative; width: 98%;text-align: justify;padding: 18px;}
			.heading{position: relative; width: 98%;text-align: left;font-size: 22px;font-weight: bold;}
			.text{width: 95%;font-size: 16px;text-align: justify;padding: 10px 0px 10px 0px;}
			.authors{position: relative;width: 80%;margin: auto;padding: 2%;font-style: italic;text-align: center;font-size: 16px;}
			.image{width: 95%;font-size: 12px;text-align: left;}
		</style>
	</head>
	<body>
		<div class="container">
			<div class="title" id="T1">EMOTION DETECTION FROM SPEECH</div>

			<div class="authors">

				<!-- Start edit here  -->
				<p>Deepak Kumar, Roll No.: 150108008, Branch: EEE</p>; &nbsp; &nbsp;
				<p>Harsh Sinha, Roll No.: 150108014, Branch: EEE</p>; &nbsp; &nbsp;
				<p>Kapil Kumar, Roll No.: 150108016, Branch: EEE</p>; &nbsp; &nbsp;
				<p>Vishal Kumar Sinha, Roll No.: 150108042, Branch: EEE</p>; &nbsp; &nbsp;
				<!-- Stop edit here -->

			</div>


			<div class="section">
				<div class="heading" id="T2">Abstract</div>
				<div class="text">
                                   The ability to understand the emotions in a speech is what seperates a human from a machine.
				   Emotion is one of the biggest part of a speech which truly confers the meaning of speech. Thus, With
				   the increasing mechanisation of the modern world, the human-machine interaction is one of the most looke
				   after research in today's scientific community. Our projects aim at classifying a speech into based on the
				   their emotions by extracting different features.
					

				</div>
			</div>

			<div class="section">
				<div class="heading id="T3">1. Introduction</div>
				<div class="text">

					<!-- Start edit here  -->
					 Emotion classification is one of the most challenging tasks in a speech signal processing domain. The problem
of speaker or speech recognition becomes relatively an easier one when compared with recognizing emotion from
speech. Sound signal is one of the main medium of communication and it can be processed to recognize the speaker,
speech or even emotion. The basic principle behind emotion recognition lies with analysing the acoustic difference that
occurs when uttering the same thing under different emotional situations. The Growing use of Machines and the necessity for a Man-Machine interaction like human
					interaction is what that have motivated the researchers to work on this project. The
					Whole idea behind the project is to develop a system through which a machine can understand 
					the emotion of a human's speech.
					
					Emotion detection is a tool which will define the course of a new age of interdependence of 
					human and machine.
					
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="heading" id="T4">1.1 Introduction to Problem</div>
					<div class="text">

						<!-- Start edit here  -->
						The project aims at classification of basic human emotions like sad, happy, neutral 
						and angry in a speech. The essence of project rely upon the selection of features of
						speech that was responsible for various human emotions. 
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading" id="T5">1.2 Figure</div>
					<div class="image">

						<!-- Start edit here  -->
	
						<img src="project1.jpg" align="center" alt="Robot Hugging a Guy" width="600px" height=""/>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading" id="T6">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						We have taken help of various research papers for the selection of features and further proceedings.
						One of them is the "CS239 stanford emotion detection from speech" and other one is "Survey on speech emotion 
						recognition: Features, classification schemes and databases by university of cairo"
						
						As gathered from reading various papers written on this topic, There are many features specifically
						needed for this project like LFPC, LPCC along with the common features like MFCC. There has been a lot
						of success in many areas of having a machine with emotion detection feature but there are still reasearch
						going on and researchers are bidding on achieving the man-machine interaction via speech very soon.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading" id="T7">1.4 Proposed Approach</div>
					<div class="text">

						<!-- Start edit here  -->
						We will start with collecting the sample data from various sources. Then, we are planning on finding the particular
						features that were important and necessary for emotion detection (found after reading various research papers). We will now 
						extract features from scratch. We will extract some short-term features like MFCC, ZCR and others. We will
					        check the accuracy of our classifier for different combination of features and then selected the combination with best
					        accuracy. After that, we will train the classifier (KNNs and SVM) with our processed data and find out the accuracy.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading" id="T8">1.5 Report Organization</div>
					<div class="text">
                                        <ul>
						<!-- Start edit here  -->
					 Report is orgazined in following way : <br /> <br />
						
						<li>Title and Group information<a href="#T1">[0]</a></li>
						<li> Abstract<a href="#T2">[0.1]</a></li>
                                                <li>Introduciton<a href="#T3">[1]</a></li>
						<ul>
								 <li> Introduction to problem<a href="#T4">[1.1]</a>
		                                                 <li>Figure<a href="#T5">[1.2]</a>
							         <li>Literature Review<a href="#T6">[1.3]</a>
		                                                  <li>Proposed approach<a href="#T7">[1.4]</a>
			                                          <li>Report Organization<a href="#T8">[1.5]</a></li>
						</ul>
								<li>Proposed Approach<a href="#T9">[2]</a></li>
								<ul> <li>Data Collection<a href="#T10">[2.1]</a></li>
												      <li>Preprocessing and Feature selection<a href="#T11">[2.2]</a>
						     <li>Classification<a href="#T12">[2.3]</a> 
			
						</ul>
										     <li>Experments and Results<a href="#T13">[3]</a></li>
						 <ul>
															     <li>Dataset Description<a href="#T14">[3.1]</a></li>
							<li>Discussion<a href="#T15">[3.2]</a></li>
						</ul>
						
										    <li>Conclusions<a href="#T16">[4]</a></li>
						 <ul>
							 <li> Summary<a href="#T17">[4.1]</a>
										   <li>Future Extensions<a href="#T18">[4.2]</a>
						</ul>
						
						</ul>
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading" id="T9">2. Proposed Approach</div>
				<div class="text">

					<!-- Start edit here  -->
					The Approach is majorly divided into three steps :
					<ol>
					            <li> Data Collection
					            <li> Preprocessing and Feature selection
					            <li>Classification
					</ol>
					<!-- Stop edit here -->
					<div class="subsection">
						<div class="heading" id="T10">2.1 Data Collection </div>
						<div class="text">
						    We have collected data from various online sources which was recorded by 
						    semi-professional actors <a href=#T14>[dataset]</a>and we have also made some data ourselves too but
						    major training data is the online one.
						</div>
					</div>
					<div class="subsection">
						<div class="heading" id="T11">2.2 Preprocessing and Feature selection</div>
						<div class="text">
							We selected the dataset which was noise free and of high quality type. So, we did not need much of
						        pre-processing for our audio clips. So, we went on extracting features from our .Wav files. We 
							extracted a total of 42 features via coding in python. The features are : ZCR, Energy, Entropy of Energy,
							Spectral centroid, Spectral Spread, Spectral Entropy, Spectral Flux, Spectral Roll-off, MFCCs, Chroma Vectors
						        Chromo-deviation. After that, we normalized these features and got a processed .CSV file. Then we tested the 
						        combination of these features and saw the result of it on the classifier accuracy and chose the best combination
							of the features. The codes written for these extraction are <a href="https://github.com/harsh4723/Emotion-Detection" target="#">here</a>
							The codes for classification by KNNs are <a href="https://github.com/harsh4723/KNN-on-iris-data" target="#">here</a>
							
							
					
						</div>
					</div>
					<div class="subsection">
						<div class="heading" id="T12">2.3 Classification</div>
						<div class="text">
						        After reading out papers, We got to know that SVMs and KNNs are best algorithms for classifications.
						       <center><h3>SVM</h3></center>
								 <div class="text">
									In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall
										  </div>            </div>
										  <center><h3>KNN</h3></center>
										  <div class="text">
												   In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.
In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.
												   </div>
												   
					</div>
				     
				</div>
			</div>

			<div class="section">
				<div class="heading" id="T13">3. Experiments &amp; Results</div>
				<div class="subsection">

					<div class="heading" id="T14">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						We used two datasets.<br>
						<b>Toronto emotional speech set (TESS) collection</b> was used.
						Target words were spoken in the carrier phrase <I>"Say the word _____"</I> by an actress aged 26 and recordings were 
	                                        made of the set portraying each of three emotions(anger, happiness and sadness).<br>.Actress speaks English as her first language and has
						musical	training.<br>
                                                Authors: Kate Dupuis, M. Kathleen Pichora-Fuller
                                                University of Toronto, Psychology Department, 2010.

						<a href="https://tspace.library.utoronto.ca/handle/1807/24487" target="#">Dataset1</a>
						<br>Another dataset we used was <b>Fullon Emotional Speech Synthesis collection</b>.
						<a href="http://homepages.inf.ed.ac.uk/ghofer/" target="#">Dataset2</a>
						<br>We recorded some of our own samples.<a href="https://github.com/harsh4723/Emotion-Detection/tree/master/Datasets/selfgeneratedaudio" target="#">Here</a>

						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading" id="T15">3.2 Discussion and Results</div>
					<div class="text">

						<!-- Start edit here  -->
						Initially while starting the project, we went on reading many research papers and discussed about the various features 
						which was being used and the research which is currently going on. We first trained the data using SVM and KNN.The data was in
							 csv format with 34 features .Then we wrote a code for testing various combinations of features.We found that removing chroma deviation 
							 and some other  chroma features gave a better result. The first  code was giving accuracy based on a segment of a clip
							 .Our Second Code was to find the classification of a whole clip and it then gave acuracy close to 90%.The concept was that in a clip there are multiple segment
							 each classified as different label and we took maximum count of the label for each clip and hence we got better accuracy.
						
							 <!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading" id="T16">4. Conclusions</div>
				<div class="subsection">
					<div class="heading" id="T17">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						The aim of our project was to classify emotions from speech. We started out with data collection from various sources.
						We tried some online datasets which are mentioned and we have tried collecting some datasets ourselves too. After the
						Data collection, we went for pre-processing and feature selection. In pre-processing we used Mean Normalization and we 
					        also tried scaling. In feature selection, we wrote codes to extract 34 short-term features which included MFCC, ZCR and 
					        others and we tried out other different features by applying it to our classifier. Then, we went on training our classifier
					        We trained SVM firstly and then KNN. We got better results with SVM, so we stick with it. Finally we tested data on our
						classifier.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading" id="T18">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						
						We have extracted very basic features in the this project due to time-constraint. So, we are thinking of extracting
						some more features like LFPC and Formants and then implement it. 
							 
					        This project was a single level classification where we were finding a single emotion in one clip of sound. We have 
					        thought of extending it further and build a multi-label classification system, where a clip will be classified into
					        different emotions content which it will contain. We have worked a bit and build a multi-label KNN for this. <a href="https://github.com/harsh4723/Multi-label-knn" target="#">This is the
					        we have worked on.</a>
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

		</div>
	</body>
</html>
